\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements
\usepackage[final]{pdfpages}
\usepackage[parfill]{parskip}
\usepackage{bm}

\usepackage[utf8]{inputenc}
% Palatino
\usepackage{mathpazo}
%\usepackage{times} % Uncomment to use the Times New Roman font
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage[letterpaper, portrait]{geometry}
 \geometry{
 letterpaper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 bottom=20mm
 }
 \frenchspacing
\setlength{\columnsep}{10mm}
\usepackage{sectsty}
\sectionfont{\fontsize{12}{0}\selectfont}

\usepackage{amsmath,amsfonts,amssymb}

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

\newcommand{\bsig}{\bigg{\Sigma}}
%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{A Reinforcement Learning Approach for Species One Counterpoint Composition}

\author{\textsc{Nick Walker}}
\date{}

\begin{document}
\twocolumn
	\maketitle % Insert the title, author and date


	%----------------------------------------------------------------------------------------
	%	ABSTRACT
	%----------------------------------------------------------------------------------------

	\begin{abstract}
	Automatic composition systems often depend on large corpora of existing compositions. In some cases, it is possible to describe the parameters of a composition quite well, and we may desire to create generative music models using these rules alone. We formulate a restricted music composition task as a Markov decision process suitable for use in a reinforcement learning context, and demonstrate that an agent can indeed produce compositions that approach passable with only experience gained by direct interaction with the environment. 
	\end{abstract}

	%----------------------------------------------------------------------------------------
	%	SECTION 1
	%----------------------------------------------------------------------------------------

	\section{Introduction}
    Automatic composition research aims to create systems that are capable of producing pleasing music with minimal human intervention, something of broad commercial and artistic interest. While many researchers have applied artificial intelligence methods to composition tasks, relatively little work has evaluated reinforcement learning methods in these domains. One contributing factor is the difficulty of formulating a reward function and state-action representation that both make learning tractable and enable musically interesting output.

    \textit{Counterpoint}, a composition sub task that involves the creation of multiple independent but musically entwined voices, is one area of music that is amenable to a reinforcement learning formulation. Species counterpoint is a series of five constrained versions of counterpoint which have well defined rules that can be translated into a reward function \cite{Kostka2012}. The relatively short duration of these compositions makes their state-action space tractable for value function methods.

    This work formulates species one counterpoint composition as a Markov decision process (MDP). We evaluate the performance of two MDP solving methods from reinforcement learning on different parameterizations of our formulation, and provide a qualitative characterization of learned policies. 

   	%----------------------------------------------------------------------------------------
    %	SECTION 2
    %----------------------------------------------------------------------------------------
	\section{Related Work}
    Researchers have applied a variety of artificial intelligence methods to algorithmic composition tasks.

    The use of genetic algorithms for algorithmic composition, and in particular musical imitation tasks, has been explored extensively over the last three decades \cite{Miranda2007}. Many researchers have had success with species counterpoint fitness functions, but the difficulty of automatic assessment in more sophisticated composition tasks has spawned many \textit{interactive genetic algorithm} based systems, which use human input to measure population performance \cite{Fernandez2013}. The most significant example is Biles' \textit{GenJam} system, which processes human provided binary clicker feedback into short- and mid-term evaluative feedback \cite{Biles94}. Our work can be seen as following in footsteps of early evolutionary music researchers in attempting to demonstrate the feasibility of using a species counterpoint objective. More recent works that integrate human feedback go beyond what we explore, but do serve to demonstrate to the reader that short term evaluative feedback can produce musically pleasing outcomes.
    
    Recent research has explored the use of multiple fitness functions. Scirea implemented a system to optimize three fitness functions which encoded harmonic and melodic characteristics \cite{Scirea2016}. The approach evaluated generations by a set of fixed feasibility constraints, then performed multiobjective optimization on the feasible population to produce melodies. The use of multiple explicit objective functions promoted the generation of pieces that only partially satisfied each of them. Our approach collapses what could be seen as multiple distinct objectives within the composition task into a single reward function. Because Scirea's work blends several general musical principles, a qualitative evaluation was required to validate that the output produced had certain properties. In contrast, this work uses well defined tasks and eschews most issues of subjective musical taste, focusing instead on learning performance as its main evaluation metric.

 	There have been many constraint based approaches to the algorithmic composition task. Boenn used Answer Set Programming to implement the rules of species counterpoint in \textit{Anton}. The user may elect to provide additional constraints, in the form of notes or a key, then a composition can be generated by selecting a random solution from the answer set defined by both his and the system's constraints \cite{Boenn2008}. In general however, modeling composition as a strict constraint satisfaction is not only inflexible, but also computationally unfeasible. Some work has investigated parameterized soft constraints that can be layered over more conventional generative models. Papadopoulos et. al. combined Markov-chains built from a corpus with parametric meter and harmony constraints that encouraged higher level structure \cite{Papadopoulos2016}. Their method allows human input in the form of parameter choices and corpus selection. Our approach uses similar constraint notions to define a reward function, but is otherwise dissimilar in that it allows an agent to learn the task.
    
    There have been several direct applications of reinforcement learning to composition. Work by Smith used multiple Adaptive Resonance Theory (ART) neural networks, which implement a self organization scheme reflective of human cognition, to construct an agent that sought novel pitch and phrasal variations. The agent was intrinsically motivated, learning a reward signal that encouraged increases in model entropy \cite{Smith2012}. Work by Cont uses Dyna-Q with a cognitively inspired reward and update scheme. During training, the reward signal reinforces states that are prefixes of musical sequences in a corpus, and during evaluation it penalizes states that are prefixes to the most recently composed notes \cite{Cont2007}. Both works tapped theories of music cognition to create their reward function, hoping to promote emergent creative behavior, but ultimately struggle to generate palatable output. In contrast, this work uses the well defined rules of the species task as its reward function. This approach is not as general, but does not rely on solving creativity in order to generate music. Another work by the Google Magenta group has used reinforcement learning to tune an LSTM music generation model \cite{Jaques2016}. Their approach relies on having a trained model already available. We attempt to learn directly from a set of rules.

	The work most similar to this paper is Phon-Amnuasuk's agent for two-part tonal counterpoint \cite{Phon-Amnuaisuk2009}. Their approach used a tabular representation where states were previous notes and actions were interval movements relative to the previous pitch. The agent observes states and composes the next notes for each line jointly in one timestep. Our work uses raw pitch values, and explores providing different history lengths. Further, our work explores the use value function approximation methods with this representation. Their reward function implemented a number of musical heuristics, but because their agent operates without the restrictions of species counterpoint, there is no simple, objective measurement of its musical ability. In contrast, our work uses species one as a well defined benchmark. 

   	%----------------------------------------------------------------------------------------
    %	SECTION 3
    %----------------------------------------------------------------------------------------
	\section{Background}
    \subsection{Music}
    Music can be described by notes on a staff. A staff is graph with pitch increasing along the y-axis and time advancing along the x-axis. Each note describes a pitch, measured in tones, and a duration, measured in beats. The development of a single line of music horizontally across time is called melody, while the interaction of pitches vertically is called harmony. Two pitches, either vertically or horizontally create an interval. Intervals have aural qualities as a byproduct of human musical cognition; the most stable are said to be perfect consonances, slightly unstable but pleasing intervals are said to be imperfect consonances, while other intervals are said to be dissonant.
    
    \subsection{Species Counterpoint}
    
    Counterpoint is the interaction of two or more independent but interconnected musical lines. The analysis and composition of counterpoint is a challenging task for humans. Historically, musicians have been introduced to topic through species counterpoint, a series of restricted composition tasks invented by baroque composer Johann Joseph Fux in 1725 \cite{Davidian2015}. The first four species apply different constraints on the composer's use of specific intervals, and completely prescribe rhythm. The fifth task, florid counterpoint, allows the composer to freely transition between the four preceding species, introducing more rhythmic and stylistic variation. The resulting compositions are characteristic of 16th century sacred music, in the tradition of Giovanni Pierluigi da Palestrina, whose compositions Fux modeled species counterpoint after.
    
    This work examines the first species task, which constrains the composer to 11 notes per line and prescribes a fixed set of pitches for each voice.
    \begin{figure}
        \includegraphics[width=8cm]{ranges.pdf}
        \caption{C major scale depicting a portion of the vocal ranges available. Full ranges are soprano [C4, G5] and tenor [C3, C5].}
    \end{figure}
    
    \subsection{Markov Decision Process}
    A Markov decision process consists of a set $\mathcal{S}$ of states, a set $\mathcal{A}$ of actions, a function $P$ which describes the probability of transitioning between states, a function $R$ which describes the reward of a state-action-state tuple, and a discount factor $\gamma$ which balances future rewards against immediate rewards. In a reinforcement learning setting, an agent attempts to maximize its expected cumulative discounted reward by learning a value function $Q(s,a)$ which provides the expected return of taking an action in a state and acting optimality thereafter. From the value function, an optimal policy $\pi\/*$ can be extracted.
    
    In our experiments we use a tabular form of True Online Sarsa($\lambda$) as well as Sarsa with CMACS tile coding \cite{VanSeijen2016}\cite{Singh1996}\cite{Sutton1998}. 
 
 
    %----------------------------------------------------------------------------------------
    %	SECTION 4
    %----------------------------------------------------------------------------------------   
    \section{Formulation}
    
    We consider species one counterpoint as an episodic MDP and provide definitions for $\mathcal{R}$, $\mathcal{S}$ and $\mathcal{A}$. Due to the small time horizon and fixed dynamics of our task, we select $\gamma$ to be 1.
    
    \subsection{Reward Function}
    Species counterpoint's strict rules can be translated into a grading rubric. We manually assigner each type of rule violation into one of three categories: minor, major, and grave, corresponding to their subjective severity. 
    
    This rubric is used to implement a \textsc{grade} routine, which assigns scores to compositions, either complete or in progress. When grading partial compositions, only penalties that cannot be corrected are assigned. For instance, an incomplete composition is not be penalized for failing to end in a particular way because it is still possible for a continuation of the composition to end correctly. Similarly, even though the empty composition is not good counterpoint, \textsc{grade} will return 0 because it is still possible to correct the composition.
    
    \textsc{grade} is used to construct a reward function for the task: for each action, the agent is given a reward equal to the resulting decrease in the composition grade. 
    \begin{equation}
    r(s,a,s') = \textsc{grade}(s') - \textsc{grade}(s)
    \end{equation}
    
    This reward function has the useful property that the cumulative reward at time $e$ is equal to the grade of the composition at $e$. This means that it provides one-step feedback but does not modify the optimal policy, which should compose music with a minimum of penalties. This form of reward function bears resemblance to reward shaping \cite{Li2011}.
    
    \begin{align*}
    \sum_{t=1}^{e} R_t &= \sum_{t=1}^{e} \textsc{grade}(s_t) - \textsc{grade}(s_{t-1})\\
    &= \textsc{grade}(s_e)
    \end{align*}
    \begin{table}
        \begin{tabular}{p{6cm}r}
            \toprule
            Element & Penalty\\
            \midrule
            Parallel, similar or oblique motion & -1\\
            Leap & -1\\
            Voice crossing & -1\\
            Pitch used more than three times & -1\\
            Dissonant harmonic interval & -5\\
            Dissonant melodic interval & -5\\
            Two or more harmonic perfect consonances in a row & -5\\
            More than three of the same harmonic interval in a row & -5\\
            Excessive range & -5\\
            Failure to surround leap with counter-stepwise motion& -5\\
            Failure to begin with tonic unison & -10\\
            Failure to end with perfect authentic cadence & -10\\
            
            
            \bottomrule
        \end{tabular}
        \caption{Species one grading rubric}
    \end{table}
    \subsection{State- and Action-spaces}
    
    The agent moves left-to-right composing vertical moments, which occur at beat thresholds where at least one voice requires a new note. In the species one task, there are 11 vertical moments, each requiring a new note for both voices. Because the agent knows the permissible pitches according to the scale and the voice ranges a prior\`i, there are 12 pitches available per voice. The agent can select any pair of pitches, including duplicates, as its action.
   
    State representation for composition is complicated by the fact that important structure can emerge at arbitrary time scales throughout the task: notes composed at the beginning of a piece can affect the value of notes near the end, but it is infeasible to provide the agent arbitrary amounts of preceding composition information. Instead, we provide a $k$-history of vertical moments. Due to the brevity of the species tasks, and their strong emphasis on local behavior, we hypothesize that this is sufficient to facilitate learning near-optimal policies.
    
    Species one emphasizes unique behavior at the beginning and end of compositions. Because we are not including a full history, it is necessary to include the beat number of the last vertical moment composed in the state so that the agent can be aware of when the composition is ending.
    
    If $l$ is the number of beats in the composition and $k$ is the history length, the size of a tabular representation of the state- and action-spaces is given by
    \begin{equation}
        |\mathcal{S}| = l \cdot 12^{2k} \hspace{0.5cm} |\mathcal{A}| = 12^2
    \end{equation}
  
    We also consider an approximate form of the value function to combat the difficulty of learning the large space. In this formulation, each pitch in the action and history, as well as the beat number, is represented as a scalar entry in a vector. The number of elements in this vector is $2k + 3$.
    
    
   	%----------------------------------------------------------------------------------------
    %	SECTION 5
    %----------------------------------------------------------------------------------------
	\section{Experimental Results}

    The agent learns with an $\epsilon$-greedy exploration policy. $\epsilon$ begins as 0.3 and decays to 99.999\% of its former value at the end of each episode. All parameter values were determined by informal experimentation. The policy is assessed periodically by freezing the value function and evaluating a fully greedy composition. Plots show 90\% confidence intervals. \footnote{All source code used for our experiments is available at github.com/nickswalker/counterpoint-reinforcement-learning}
    
   	\begin{figure}
        \begin{subfigure}{\linewidth}
            \includegraphics[width=8cm, keepaspectratio]{figure0.pdf}
            \caption{Tabular True Online Sarsa($\lambda$), $\lambda$ = 0.5, $\alpha$ = 0.6. Note the range of the x-axis.}
                    \vspace{1cm}
        \end{subfigure}
        
        \begin{subfigure}{\linewidth}
            \includegraphics[width=8cm, keepaspectratio]{figure1.pdf}
            \caption{Sarsa with CMAC tile coding $\alpha$ = 0.6}
                    \vspace{1cm}
        \end{subfigure}
        
        \begin{subfigure}{\linewidth}
            \includegraphics[width=8cm, keepaspectratio]{figure2.pdf}
            \caption{Sarsa with CMAC tile coding, no beat number $\alpha$ = 0.6}
                    \vspace{1cm}
        \end{subfigure}
    \end{figure}
        

    \begin{figure*}
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\textwidth]{early.pdf}
        \caption{Early evaluation. Grade -115}
        \vspace{1cm}
    \end{subfigure}
    
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\textwidth]{mid.pdf}
        \caption{Intermediate evaluation. Grade -45}
        \vspace{1cm}
    \end{subfigure}
    
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\textwidth]{late.pdf}
        \caption{Late evaluation. Grade -26}
        \vspace{1cm}
    \end{subfigure}
    \caption{Compositions taken from the $k$ = 1 tabular agent's policy at different points of learning. Early compositions are essentially random. Later, better scoring compositions emerge, and exhibit the smooth, stepwise contour that is characteristic of species one compositions. However, the agent's compositions are hampered by excessive repetition. This is likely due to the limited history information available. WAV audio files are available at cs.utexas.edu/users/nwalker/counterpoint}

    \end{figure*}

   	%----------------------------------------------------------------------------------------
    %	SECTION 6
    %----------------------------------------------------------------------------------------
	\section{Discussion}
    
    A tabular value function allows the agent to reach near optimal performance, even with $k$=1. The learned policy's compositions are penalized mostly for excessive repetition, a concept which the value function struggles to encode without direct access to additional history information. Because the agent cannot generalize, learning time is high. Experiments with higher values of $k$ failed to learn in a reasonable amount of time. This is not unexpected; the size of the state space is exponential in $k$, so the amount of training required to even experience the same states multiple times quickly grows beyond reason.
    
    Agents using value function approximation reach middling permanence quickly, but fail to reach the peak performance of the tabular agent. Though fewer interactions are needed, wall-clock learning time is not much faster than tabular methods due to the expense of maximizing over the action-value function at each timestep, which requires $12^2$ value calculations. Promoting generalization across time by removing the beat number feature speeds learning, but likely places a hard limit on asymptotic performance, as the value function cannot encode important time-dependent distinctions. We observed that adding additional features, like a preceding pitch histogram did not accelerate learning.
    
    The difference in learning speed for $k=1$ and $k=2$ using approximation is statistically significant, however further experiments are necessary to elucidate their relative asymptotic performance. It is curious that the $k=3$ agent seems to perform between the 1 and 2 agents, though it is not significant. Should this signal remain after further experiments, we speculate that a 3 history enables the agent to quickly adapt to elements of the reward signal that depend on three repetitions of a note.
    
    The size of the state-action space is a fundamental challenge with this formulation. In either the tabular setting, where it mandates significant learning time, or in the approximate setting, where it induces an expensive maximization step, we were not able to fully overcome the limitations it imposed.
    

	%----------------------------------------------------------------------------------------
	%	SECTION 7
	%----------------------------------------------------------------------------------------

	\section{Conclusion and Future Work}
    
    We have demonstrated the feasibility of learning a composition task directly from its rules in a reinforcement learning setting, and the difficulties inherent in doing so. 
    
    Several future directions are clear. This work has evaluated learned policies greedily. While it is true that learned value functions for this domain should permit stochastic policies if action-value ties are broken randomly, our evaluations are deterministic because we never see a truly converged value function. Further work should characterize learning and composition performance under a soft-max policy. The domain's model is large, but fully available. A tree search approach may enable the agent to navigate the state space more efficiently. Recent work in recurrent reinforcement learning holds the promise of automatically encoding memory information \cite{Hausknecht2015}. Future work should investigate the use of LSTMs and other recurrent neural network function approximators.

   	%----------------------------------------------------------------------------------------
    %	SECTION 8
    %----------------------------------------------------------------------------------------
        

    \section{Acknowledgments}
    
    We thank Jivko Sinapov and Elad Liebman for their helpful guidance and feedback.
	\bibliography{references}{}
	\bibliographystyle{plain}
\end{document}