@Inbook{Phon-Amnuaisuk2009,
author="Phon-Amnuaisuk, Somnuk",
editor="Leung, Chi Sing
and Lee, Minho
and Chan, Jonathan H.",
title="Generating Tonal Counterpoint Using Reinforcement Learning",
bookTitle="Neural Information Processing: 16th International Conference, ICONIP 2009, Bangkok, Thailand, December 1-5, 2009, Proceedings, Part I",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="580--589",
isbn="978-3-642-10677-4",
doi="10.1007/978-3-642-10677-4_66",
url="http://dx.doi.org/10.1007/978-3-642-10677-4_66"
}

@book{Sutton1998,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@Inbook{Boenn2008,
author="Boenn, Georg
and Brain, Martin
and De Vos, Marina
and ffitch, John",
editor="Garcia de la Banda, Maria
and Pontelli, Enrico",
title="Automatic Composition of Melodic and Harmonic Music by Answer Set Programming",
bookTitle="Logic Programming: 24th International Conference, ICLP 2008 Udine, Italy, December 9-13 2008 Proceedings",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="160--174",
isbn="978-3-540-89982-2",
doi="10.1007/978-3-540-89982-2_21",
url="http://dx.doi.org/10.1007/978-3-540-89982-2_21"
}

@Inbook{Smith2012,
author="Smith, Benjamin D.
and Garnett, Guy E.",
editor="Machado, Penousal
and Romero, Juan
and Carballal, Adrian",
title="Reinforcement Learning and the Creative, Automated Music Improviser",
bookTitle="Evolutionary and Biologically Inspired Music, Sound, Art and Design: First International Conference, EvoMUSART 2012, M{\'a}laga, Spain, April 11-13, 2012. Proceedings",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="223--234",
isbn="978-3-642-29142-5",
doi="10.1007/978-3-642-29142-5_20",
url="http://dx.doi.org/10.1007/978-3-642-29142-5_20"
}

@inproceedings{LopezdeMantaras2006,
 author = {Lopez de Mantaras, Ram\'{o}n},
 title = {Making Music with AI: Some Examples},
 booktitle = {Proceedings of the 2006 Conference on Rob Milne: A Tribute to a Pioneering AI Scientist, Entrepreneur and Mountaineer},
 year = {2006},
 isbn = {1-58603-639-4},
 pages = {90--100},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=1565082.1565089},
 acmid = {1565089},
 publisher = {IOS Press},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Artificial Intelligence, Computational Models Of Music},
} 

@article{Paiement2009,
title = {Predictive Models for Music},
author  = {Jean-Francois Paiement and Yves Grandvalet and Samy Bengio},
year  = 2009,
journal = {Connection Science},
pages = {253--272},
volume  = {21}
}

@Inbook{Scirea2016,
author="Scirea, Marco
and Togelius, Julian
and Eklund, Peter
and Risi, Sebastian",
editor="Johnson, Colin
and Ciesielski, Vic
and Correia, Jo{\~a}o
and Machado, Penousal",
title="MetaCompose: A Compositional Evolutionary Music Composer",
bookTitle="Evolutionary and Biologically Inspired Music, Sound, Art and Design: 5th International Conference, EvoMUSART 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="202--217",
isbn="978-3-319-31008-4",
doi="10.1007/978-3-319-31008-4_14",
url="http://dx.doi.org/10.1007/978-3-319-31008-4_14"
}

@incollection{Cont2007,
  TITLE = {{Anticipatory Model of Musical Style Imitation using Collaborative and Competitive Reinforcement Learning}},
  AUTHOR = {Cont, Arshia and Dubnov, Shlomo and Assayag, Gerard},
  URL = {https://hal.inria.fr/hal-00839073},
  BOOKTITLE = {{Anticipatory Behavior in Adaptive Learning Systems}},
  EDITOR = {Butz M.V. and Sigaud O. and Pezzulo G. and Baldassarre G.},
  PUBLISHER = {{Springer Verlag}},
  SERIES = {Lecture Notes in Computer Science / Artificial Intelligence (LNAI)},
  VOLUME = {4520},
  PAGES = {285-306},
  YEAR = {2007},
  PDF = {https://hal.inria.fr/hal-00839073/file/ArshiaCont_ABIALS06_Chapter.pdf},
  HAL_ID = {hal-00839073},
  HAL_VERSION = {v1},
}

@article{Roads1985,
 author = {Roads, Curtis},
 title = {Research in Music and Artificial Intelligence},
 journal = {ACM Comput. Surv.},
 issue_date = {June 1985},
 volume = {17},
 number = {2},
 month = jun,
 year = {1985},
 issn = {0360-0300},
 pages = {163--190},
 numpages = {28},
 url = {http://doi.acm.org/10.1145/4468.4469},
 doi = {10.1145/4468.4469},
 acmid = {4469},
 publisher = {ACM},
 address = {New York, NY, USA},
}
@article{Baird1993,
 ISSN = {01489267, 15315169},
 URL = {http://www.jstor.org/stable/3680871},
 author = {Bridget Baird, Donald Blevins, Noel Zahler},
 journal = {Computer Music Journal},
 number = {2},
 pages = {73-79},
 publisher = {The MIT Press},
 title = {Artificial Intelligence and Music: Implementing an Interactive Computer Performer},
 volume = {17},
 year = {1993}
}
@inproceedings{Collins2008,
  title={Reinforcement learning for live musical agents},
  author={Collins, Nick},
  booktitle={Proceedings of the International Computer Music Conference (ICMC), Belfast},
  year={2008}
}

@incollection{Franklin2002,
 author = {Franklin, Judy A. and Manfredi, Victoria U.},
 chapter = {Nonlinear Credit Assignment for Musical Sequences},
 title = {Computational Intelligence and Applications},
 editor = {Abraham, Ajith and Nath, Baikunth and Sambandham, M. and Saratchandran, P.},
 year = {2002},
 isbn = {0-9640398-0-X},
 pages = {245--250},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=989710.775004},
 acmid = {775004},
 publisher = {Dynamic Publishers, Inc.},
 address = {Atlanta, GA, USA},
}

@INPROCEEDINGS{Widmer97,
    author = {Gerhard Widmer},
    title = {On the Potential of Machine Learning for Music Research},
    booktitle = {Readings in Music and Artificial Intelligence},
    year = {1997},
    pages = {69--84},
    publisher = {Harwood Academic Publishers}
}

@article{Fernandez2013,
 author = {Fern\'{a}ndez, Jose David and Vico, Francisco},
 title = {AI Methods in Algorithmic Composition: A Comprehensive Survey},
 journal = {Journal of Artificial Intelligence Res.},
 issue_date = {October 2013},
 volume = {48},
 number = {1},
 month = oct,
 year = {2013},
 issn = {1076-9757},
 pages = {513--582},
 numpages = {70},
 url = {http://dl.acm.org/citation.cfm?id=2591248.2591260},
 acmid = {2591260},
 publisher = {AI Access Foundation},
 address = {USA},
}
@phdthesis{Thom2001,
 author = {Thom, Belinda Carol},
 title = {Bob: An Improvisational Music Companion},
 year = {2001},
 isbn = {0-493-65333-3},
 note = {AAI3051011},
 publisher = {Carnegie Mellon University},
 address = {Pittsburgh, PA, USA},
}
@inproceedings{Loughran2016
 author = {Loughran, R\'{o}is\'{\i}n and Mcdermott, James and O'Neill, Michael},
 title = {Grammatical Music Composition with Dissimilarity Driven Hill Climbing},
 booktitle = {Proceedings of the 5th International Conference on Evolutionary and Biologically Inspired Music, Sound, Art and Design - Volume 9596},
 year = {2016},
 isbn = {978-3-319-31007-7},
 pages = {110--125},
 numpages = {16},
 url = {http://dx.doi.org/10.1007/978-3-319-31008-4_8},
 doi = {10.1007/978-3-319-31008-4_8},
 acmid = {2954535},
 publisher = {Springer-Verlag New York, Inc.},
 address = {New York, NY, USA},
 keywords = {Algorithmic composition, Grammar, Hill-climbing},
}
@Inbook{Papadopoulos2016,
author="Papadopoulos, Alexandre
and Roy, Pierre
and Pachet, Fran{\c{c}}ois",
editor="Rueher, Michel",
title="Assisted Lead Sheet Composition Using FlowComposer",
bookTitle="Principles and Practice of Constraint Programming: 22nd International Conference, CP 2016, Toulouse, France, September 5-9, 2016, Proceedings",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="769--785",
isbn="978-3-319-44953-1",
doi="10.1007/978-3-319-44953-1_48",
url="http://dx.doi.org/10.1007/978-3-319-44953-1_48"
}

@book{Miranda2007,
 author = {Miranda, Eduardo Reck and Biles, John Al},
 title = {Evolutionary Computer Music},
 year = {2007},
 isbn = {1846285992},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
}

@book{Kostka2012,
 author = {Miranda, Eduardo Reck and Biles, John Al},
 title = {Tonal Harmony},
 year = {2012},
 isbn = {0078025141},
 publisher = {McGraw-Hill Education},
 address = {New York, NY, USA},
}

@book{Davidian2015,
    author = {Teresa Davidian},
    title = {Tonal Counterpoint for the 21st-Century Musician: An Introduction},
    year = {2015},
    isbn = {9781442234598},
    publisher = {Rowman and Littlefield},
    address = {New York, NY, USA},
}

@INPROCEEDINGS{Biles94,
    author = {John Biles},
    title = {GenJam: A Genetic Algorithm for Generating Jazz Solos},
    booktitle = {},
    year = {1994},
    pages = {131--137}
}

@techreport{Eck2002,
    author = {Eck, Douglas and Schmidhuber, Juergen},
    title = {A First Look at Music Composition Using LSTM Recurrent Neural Networks},
    year = {2002},
    source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aidsia_ch%3Ancstrl.idsia.ch%2F%2FIDSIA-07-02},
    publisher = {Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale},
} 

@Article{Watkins1992,
    author="Watkins, Christopher J.C.H.
    and Dayan, Peter",
    title="Technical Note: Q-Learning",
    journal="Machine Learning",
    year="1992",
    volume="8",
    number="3",
    pages="279--292",
    abstract="{\$}{\$}{\backslash}mathcal{\{}Q{\}}{\$}{\$}                -learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
    issn="1573-0565",
    doi="10.1023/A:1022676722315",
    url="http://dx.doi.org/10.1023/A:1022676722315"
}

@Article{Singh1996,
    author="Singh, Satinder P.
    and Sutton, Richard S.",
    title="Reinforcement learning with replacing eligibility traces",
    journal="Machine Learning",
    year="1996",
    volume="22",
    number="1",
    pages="123--158",
    abstract="The eligibility trace is one of the basic mechanisms used in reinforcement learning to handle delayed reward. In this paper we introduce a new kind of eligibility trace, thereplacing trace, analyze it theoretically, and show that it results in faster, more reliable learning than the conventional trace. Both kinds of trace assign credit to prior events according to how recently they occurred, but only the conventional trace gives greater credit to repeated events. Our analysis is for conventional and replace-trace versions of the offline TD(1) algorithm applied to undiscounted absorbing Markov chains. First, we show that these methods converge under repeated presentations of the training set to the same predictions as two well known Monte Carlo methods. We then analyze the relative efficiency of the two Monte Carlo methods. We show that the method corresponding to conventional TD is biased, whereas the method corresponding to replace-trace TD is unbiased. In addition, we show that the method corresponding to replacing traces is closely related to the maximum likelihood solution for these tasks, and that its mean squared error is always lower in the long run. Computational results confirm these analyses and show that they are applicable more generally. In particular, we show that replacing traces significantly improve performance and reduce parameter sensitivity on the ``Mountain-Car'' task, a full reinforcement-learning problem with a continuous state space, when using a feature-based function approximator.",
    issn="1573-0565",
    doi="10.1007/BF00114726",
    url="http://dx.doi.org/10.1007/BF00114726"
}

@InProceedings{Hausknecht2015,
    author = {Matthew Hausknecht and Peter Stone},
    title = {Deep Recurrent Q-Learning for Partially Observable MDPs},
    booktitle = {AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents (AAAI-SDMIA15)},
    location = {Arlington, Virginia, USA},
    month = {November},
    year = {2015},
    abstract={
    Deep Reinforcement Learning has yielded proficient controllers for
    complex tasks. However, these controllers have limited memory and rely
    on being able to perceive the complete game screen at each decision
    point. To address these shortcomings, this article investigates the
    effects of adding recurrency to a Deep Q-Network (DQN) by replacing
    the first post-convolutional fully-connected layer with a recurrent
    LSTM. The resulting Deep Recurrent Q-Network (DRQN), although
    capable of seeing only a single frame at each timestep, successfully
    integrates information through time and replicates DQN's performance
    on standard Atari games and partially observed equivalents featuring
    flickering game screens. Additionally, when trained with partial
    observations and evaluated with incrementally more complete
    observations, DRQN's performance scales as a function of
    observability. Conversely, when trained with full observations and
    evaluated with partial observations, DRQN's performance degrades less
    than DQN's. Thus, given the same length of history, recurrency is a
    viable alternative to stacking a history of frames in the DQN's input
    layer and while recurrency confers no systematic advantage when learning
    to play the game, the recurrent net can better adapt at evaluation
    time if the quality of observations changes.
    },
}
